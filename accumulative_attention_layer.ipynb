{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanglinzi/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.8' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import dilated_causal_convolution_layer\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTimeSeries(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Official implementation of paper \"Modelling Long- and Short-term Multi-dimensional Patterns in Predictive Maintenance with Accumulative Attention\"\n",
    "    \n",
    "    dilated_causal_convolution_layer parameters:\n",
    "        in_channels: the number of features per time point\n",
    "        out_channels: the number of features outputted per time point\n",
    "        kernel_size: k is the width of the 1-D sliding kernel\n",
    "        \n",
    "    nn.Transformer parameters:\n",
    "        d_model: the size of the embedding vector (input)\n",
    "    \n",
    "    PositionalEncoding parameters:\n",
    "        d_model: the size of the embedding vector (positional vector)\n",
    "        dropout: the dropout to be used on the sum of positional+embedding vector\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TransformerTimeSeries,self).__init__()\n",
    "        self.input_embedding = dilated_causal_convolution_layer.context_embedding(2, 256, 9)\n",
    "        self.positional_embedding = torch.nn.Embedding(512,256)\n",
    "\n",
    "        \n",
    "        self.decode_layer = torch.nn.TransformerEncoderLayer(d_model=256,nhead=8)\n",
    "        self.transformer_decoder = torch.nn.TransformerEncoder(self.decode_layer, num_layers=3)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(256,1)\n",
    "        \n",
    "    def forward(self,x,y,attention_masks):\n",
    "        \n",
    "        # concatenate observed points and time covariate\n",
    "        # (B*feature_size*n_time_points)\n",
    "        z = torch.cat((y.unsqueeze(1),x.unsqueeze(1)),1)\n",
    "\n",
    "        # input_embedding returns shape (Batch size,embedding size,sequence len) -> need (sequence len,Batch size,embedding_size)\n",
    "        z_embedding = self.input_embedding(z).permute(2,0,1)\n",
    "        \n",
    "        # get my positional embeddings (Batch size, sequence_len, embedding_size) -> need (sequence len,Batch size,embedding_size)\n",
    "        positional_embeddings = self.positional_embedding(x.type(torch.long)).permute(1,0,2)\n",
    "        \n",
    "        input_embedding = z_embedding+positional_embeddings\n",
    "        \n",
    "        transformer_embedding = self.transformer_decoder(input_embedding,attention_masks)\n",
    "\n",
    "        output = self.fc1(transformer_embedding.permute(1,0,2))\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 4500*48 fx: 4500*48\n",
      "x: 500*48 fx: 500*48\n",
      "x: 1000*48 fx: 1000*48\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.time_series_prepocess(t0,4500)\n",
    "validation_dataset = datasets.time_series_prepocess(t0,500)\n",
    "test_dataset = datasets.time_series_prepocess(t0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "validation_dl = DataLoader(validation_dataset,batch_size=64)\n",
    "test_dl = DataLoader(test_dataset,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TransformerTimeSeries().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .0005 # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dp(y_pred,y_true,q):\n",
    "    return max([q*(y_pred-y_true),(q-1)*(y_pred-y_true)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rp_num_den(y_preds,y_trues,q):\n",
    "    numerator = np.sum([Dp(y_pred,y_true,q) for y_pred,y_true in zip(y_preds,y_trues)])\n",
    "    denominator = np.sum([np.abs(y_true) for y_true in y_trues])\n",
    "    return numerator,denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,train_dl,t0=96):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    n = 0\n",
    "    for step,(x,y,attention_masks) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x.to(device),y.to(device),attention_masks[0].to(device))\n",
    "        loss = criterion(output.squeeze()[:,(t0-1):(t0+24-1)],y.to(device)[:,t0:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += (loss.detach().cpu().item() * x.shape[0])\n",
    "        n += x.shape[0]\n",
    "    return train_loss/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model,validation_dl,t0=96):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for step,(x,y,attention_masks) in enumerate(validation_dl):\n",
    "            output = model(x.to(device),y.to(device),attention_masks[0].to(device))\n",
    "            loss = criterion(output.squeeze()[:,(t0-1):(t0+24-1)],y.to(device)[:,t0:]) \n",
    "           \n",
    "            \n",
    "            eval_loss += (loss.detach().cpu().item() * x.shape[0])\n",
    "            n += x.shape[0]\n",
    "            \n",
    "    return eval_loss/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model,test_dl,t0=96):\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        observations = []\n",
    "\n",
    "        model.eval()\n",
    "        for step,(x,y,attention_masks) in enumerate(test_dl):\n",
    "            output = model(x.to(device),y.to(device),attention_masks[0].to(device))\n",
    "\n",
    "            for p,o in zip(output.squeeze()[:,(t0-1):(t0+24-1)].cpu().numpy().tolist(),y.to(device)[:,t0:].cpu().numpy().tolist()):\n",
    "           \n",
    "\n",
    "                predictions.append(p)\n",
    "                observations.append(o)\n",
    "\n",
    "        num = 0\n",
    "        den = 0\n",
    "        for y_preds,y_trues in zip(predictions,observations):\n",
    "            num_i,den_i = Rp_num_den(y_preds,y_trues,.5)\n",
    "            num+=num_i\n",
    "            den+=den_i\n",
    "        Rp = (2*num)/den\n",
    "        \n",
    "    return Rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanglinzi/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss: 3797.3092706163193 \t Validation loss: 2670.141025390625 \t R_p=0.5997030672444467\n",
      "Epoch 1: Train loss: 1854.2861497395834 \t Validation loss: 1287.182890625 \t R_p=0.407403951472577\n",
      "Epoch 2: Train loss: 872.4550033365886 \t Validation loss: 477.61826904296873 \t R_p=0.2032849813286514\n",
      "Epoch 3: Train loss: 292.14651820203994 \t Validation loss: 174.13368725585937 \t R_p=0.10485630540918982\n",
      "Epoch 4: Train loss: 115.94602427842882 \t Validation loss: 72.79549835205079 \t R_p=0.06477228327333508\n",
      "Epoch 5: Train loss: 51.66915262179904 \t Validation loss: 34.754987777709964 \t R_p=0.0459488342944672\n",
      "Epoch 6: Train loss: 25.550670500013563 \t Validation loss: 17.032090919494628 \t R_p=0.02924069037860087\n",
      "Epoch 7: Train loss: 14.261297188652886 \t Validation loss: 9.943371826171875 \t R_p=0.02357813839868287\n",
      "Epoch 8: Train loss: 9.171848890516493 \t Validation loss: 7.471519798278808 \t R_p=0.024025369055495963\n",
      "Epoch 9: Train loss: 6.270962770673964 \t Validation loss: 8.810970008850097 \t R_p=0.031899521623536\n",
      "Epoch 10: Train loss: 4.580251196543376 \t Validation loss: 3.721458755493164 \t R_p=0.01781420256012957\n",
      "Epoch 11: Train loss: 3.9178112008836536 \t Validation loss: 2.967363676071167 \t R_p=0.01683744719537397\n",
      "Epoch 12: Train loss: 3.1469961308373344 \t Validation loss: 2.59148752784729 \t R_p=0.015995931511276092\n",
      "Epoch 13: Train loss: 2.73203082508511 \t Validation loss: 3.2696127548217775 \t R_p=0.01926151278303665\n",
      "Epoch 14: Train loss: 2.646134157392714 \t Validation loss: 2.025945200920105 \t R_p=0.01484321703211262\n",
      "Epoch 15: Train loss: 2.252017942216661 \t Validation loss: 1.9419045457839965 \t R_p=0.014669873227568244\n",
      "Epoch 16: Train loss: 2.154802421040005 \t Validation loss: 3.7616815452575683 \t R_p=0.021805898391197502\n",
      "Epoch 17: Train loss: 2.1389442151387534 \t Validation loss: 1.9480369949340821 \t R_p=0.015233747393596429\n",
      "Epoch 18: Train loss: 2.0029999498791167 \t Validation loss: 2.671694488525391 \t R_p=0.017980973539033877\n",
      "Epoch 19: Train loss: 1.8872043334113227 \t Validation loss: 2.1643935985565186 \t R_p=0.015996212972729643\n",
      "Epoch 20: Train loss: 1.8160441198349 \t Validation loss: 1.6827027521133422 \t R_p=0.014085543403749698\n",
      "Epoch 21: Train loss: 1.8067754774093627 \t Validation loss: 1.8739676151275635 \t R_p=0.014932937392156254\n",
      "Epoch 22: Train loss: 1.9651651073031955 \t Validation loss: 1.663066743850708 \t R_p=0.014173723030261088\n",
      "Epoch 23: Train loss: 1.8403949536217583 \t Validation loss: 1.4844184007644654 \t R_p=0.013438029514846739\n",
      "Epoch 24: Train loss: 1.846253381093343 \t Validation loss: 1.4389275493621827 \t R_p=0.013172101685918215\n",
      "Epoch 25: Train loss: 1.6696869574652777 \t Validation loss: 1.4126775159835816 \t R_p=0.01314681372083122\n",
      "Epoch 26: Train loss: 1.7091682448916965 \t Validation loss: 1.5984089164733886 \t R_p=0.013903960169062134\n",
      "Epoch 27: Train loss: 1.7646678829193114 \t Validation loss: 1.9765768737792968 \t R_p=0.01562934560872721\n",
      "Epoch 28: Train loss: 1.9363447080188327 \t Validation loss: 1.461684186935425 \t R_p=0.013263484152609382\n",
      "Epoch 29: Train loss: 1.5801863719092475 \t Validation loss: 1.4923988857269288 \t R_p=0.01339514303193582\n",
      "Epoch 30: Train loss: 1.5290409148534139 \t Validation loss: 1.2687750358581542 \t R_p=0.01234145148645897\n",
      "Epoch 31: Train loss: 1.7179721480475532 \t Validation loss: 1.4022849073410035 \t R_p=0.01317801988372246\n",
      "Epoch 32: Train loss: 1.5920627601411608 \t Validation loss: 1.663424386024475 \t R_p=0.01414876491251606\n",
      "Epoch 33: Train loss: 1.6933330930074055 \t Validation loss: 1.3124711408615113 \t R_p=0.012643356607013028\n",
      "Epoch 34: Train loss: 1.582935431798299 \t Validation loss: 1.4115273389816285 \t R_p=0.013134372610338686\n",
      "Epoch 35: Train loss: 1.529251382086012 \t Validation loss: 1.353459966659546 \t R_p=0.012876822124083859\n",
      "Epoch 36: Train loss: 1.646146630499098 \t Validation loss: 1.6604036684036254 \t R_p=0.014234281586663452\n",
      "Epoch 37: Train loss: 1.6457126870685153 \t Validation loss: 1.2350786981582642 \t R_p=0.01228253626988461\n",
      "Epoch 38: Train loss: 1.6746593838797674 \t Validation loss: 1.3844523210525512 \t R_p=0.012997766804558197\n",
      "Epoch 39: Train loss: 1.538737730556064 \t Validation loss: 1.3418250045776368 \t R_p=0.012730007725329224\n",
      "Epoch 40: Train loss: 1.60620069609748 \t Validation loss: 1.7211856536865235 \t R_p=0.014524297191658611\n",
      "Epoch 41: Train loss: 1.6723638204998441 \t Validation loss: 1.6466179084777832 \t R_p=0.014327674256496347\n",
      "Epoch 42: Train loss: 1.5373814256456164 \t Validation loss: 1.4743886070251464 \t R_p=0.013476291097343811\n",
      "Epoch 43: Train loss: 1.572689920531379 \t Validation loss: 1.553694818496704 \t R_p=0.013663141476553252\n",
      "Epoch 44: Train loss: 1.5090854203965929 \t Validation loss: 2.3056436443328856 \t R_p=0.0167876151205567\n",
      "Epoch 45: Train loss: 1.4891063409381442 \t Validation loss: 1.7794647932052612 \t R_p=0.014622385901976077\n",
      "Epoch 46: Train loss: 1.4480053883658515 \t Validation loss: 1.8624126100540161 \t R_p=0.015299951880738943\n",
      "Epoch 47: Train loss: 1.5075226215786404 \t Validation loss: 1.4948384866714477 \t R_p=0.013448234954355019\n",
      "Epoch 48: Train loss: 1.50685805967119 \t Validation loss: 1.4639238424301146 \t R_p=0.01332562556951459\n",
      "Epoch 49: Train loss: 1.4917034680048624 \t Validation loss: 1.3424984846115113 \t R_p=0.012734888172925838\n",
      "Epoch 50: Train loss: 1.594215842988756 \t Validation loss: 1.7410109214782714 \t R_p=0.014589120012069634\n",
      "Epoch 51: Train loss: 1.454139979892307 \t Validation loss: 1.686348970413208 \t R_p=0.014486121211907534\n",
      "Epoch 52: Train loss: 1.4347326045566136 \t Validation loss: 1.7065427341461181 \t R_p=0.014718393766328684\n",
      "Epoch 53: Train loss: 1.4985408103730944 \t Validation loss: 1.3769903993606567 \t R_p=0.01301429891007267\n",
      "Epoch 54: Train loss: 1.4740485535727608 \t Validation loss: 2.1152770462036132 \t R_p=0.015963019997201372\n",
      "Epoch 55: Train loss: 1.5396854401694404 \t Validation loss: 1.7073856887817382 \t R_p=0.014384746262889476\n",
      "Epoch 56: Train loss: 1.5451874766879612 \t Validation loss: 1.4895156927108764 \t R_p=0.013439337022149484\n",
      "Epoch 57: Train loss: 1.7752486378351848 \t Validation loss: 1.198405022621155 \t R_p=0.01205371967563964\n",
      "Epoch 58: Train loss: 1.4847209515041775 \t Validation loss: 2.2470857696533204 \t R_p=0.016672805417969304\n",
      "Epoch 59: Train loss: 1.4002781738705106 \t Validation loss: 1.684804105758667 \t R_p=0.01443638378742454\n",
      "Epoch 60: Train loss: 1.429423555692037 \t Validation loss: 1.5724069452285767 \t R_p=0.013940867673514918\n",
      "Epoch 61: Train loss: 1.4662667054070366 \t Validation loss: 1.6503781766891479 \t R_p=0.014165804199537654\n",
      "Epoch 62: Train loss: 1.4811471542782253 \t Validation loss: 1.396249810218811 \t R_p=0.01313687246659597\n",
      "Epoch 63: Train loss: 1.4557785675260755 \t Validation loss: 1.9922317066192627 \t R_p=0.015691552097185348\n",
      "Epoch 64: Train loss: 1.4922772595087688 \t Validation loss: 1.311219877243042 \t R_p=0.01269750823524584\n",
      "Epoch 65: Train loss: 1.4616073670917087 \t Validation loss: 1.3562254552841186 \t R_p=0.012794219560900707\n",
      "Epoch 66: Train loss: 1.3820727615356445 \t Validation loss: 1.337622504234314 \t R_p=0.012804801031069844\n",
      "Epoch 67: Train loss: 1.4548844163682726 \t Validation loss: 1.4367711219787598 \t R_p=0.013254619055301505\n",
      "Epoch 68: Train loss: 1.4420072150760226 \t Validation loss: 1.2027791414260864 \t R_p=0.012104394938884669\n",
      "Epoch 69: Train loss: 1.497372257868449 \t Validation loss: 1.311874852180481 \t R_p=0.01257149299082161\n",
      "Epoch 70: Train loss: 1.3652081269158258 \t Validation loss: 1.2710228242874146 \t R_p=0.01247185687855607\n",
      "Epoch 71: Train loss: 1.6392872505187988 \t Validation loss: 1.6202204427719116 \t R_p=0.01413846086676813\n",
      "Epoch 72: Train loss: 1.3766616518232557 \t Validation loss: 1.479790831565857 \t R_p=0.013458960426544058\n",
      "Epoch 73: Train loss: 1.4691132506264581 \t Validation loss: 1.6107955560684204 \t R_p=0.013908220314359598\n",
      "Epoch 74: Train loss: 1.3711698666678536 \t Validation loss: 1.3442985515594483 \t R_p=0.012744908488976484\n",
      "Epoch 75: Train loss: 1.3879456784990098 \t Validation loss: 1.524414852142334 \t R_p=0.013711004182686783\n",
      "Epoch 76: Train loss: 1.3769059147304958 \t Validation loss: 2.063153688430786 \t R_p=0.0158260145070066\n",
      "Epoch 77: Train loss: 1.5688003635406493 \t Validation loss: 1.3835644178390503 \t R_p=0.012920002849532685\n",
      "Epoch 78: Train loss: 1.411970226181878 \t Validation loss: 1.3976717195510864 \t R_p=0.013130565453183321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: Train loss: 1.3752249983681573 \t Validation loss: 1.6489824514389038 \t R_p=0.014327236120659712\n",
      "Epoch 80: Train loss: 1.396236649831136 \t Validation loss: 2.507786087036133 \t R_p=0.017794621132593117\n",
      "Epoch 81: Train loss: 1.418894773695204 \t Validation loss: 1.6211523199081421 \t R_p=0.014150509906639715\n",
      "Epoch 82: Train loss: 1.3657550314797295 \t Validation loss: 1.7152012615203858 \t R_p=0.014445598937655818\n",
      "Epoch 83: Train loss: 1.3558725352817111 \t Validation loss: 2.6196584186553955 \t R_p=0.017899608062739644\n",
      "Epoch 84: Train loss: 1.40549073515998 \t Validation loss: 1.3496475772857666 \t R_p=0.012811915046972214\n",
      "Epoch 85: Train loss: 1.4001316346062553 \t Validation loss: 1.3212452440261842 \t R_p=0.012697500766598032\n",
      "Epoch 86: Train loss: 1.3466638241873847 \t Validation loss: 1.6677100067138673 \t R_p=0.014353269460440412\n",
      "Epoch 87: Train loss: 1.3485321091545952 \t Validation loss: 1.7443421087265014 \t R_p=0.014586439696879306\n",
      "Epoch 88: Train loss: 1.3473142716089885 \t Validation loss: 1.3807681827545166 \t R_p=0.012953758099504215\n",
      "Epoch 89: Train loss: 1.4338160936567519 \t Validation loss: 1.4049580669403077 \t R_p=0.013156440516461332\n",
      "Epoch 90: Train loss: 1.421033659087287 \t Validation loss: 1.6239588527679443 \t R_p=0.01416162979164591\n",
      "Epoch 91: Train loss: 1.359554450670878 \t Validation loss: 1.340528350830078 \t R_p=0.01269718379526449\n",
      "Epoch 92: Train loss: 1.4159765923817953 \t Validation loss: 1.5799179944992066 \t R_p=0.01371915260161959\n",
      "Epoch 93: Train loss: 1.39075679175059 \t Validation loss: 1.273199769973755 \t R_p=0.01247828251007848\n",
      "Epoch 94: Train loss: 1.426555093129476 \t Validation loss: 1.363143564224243 \t R_p=0.012882927700344259\n",
      "Epoch 95: Train loss: 1.343217382007175 \t Validation loss: 1.7016200094223022 \t R_p=0.014401109128171314\n",
      "Epoch 96: Train loss: 1.4596266197628445 \t Validation loss: 1.6213259468078614 \t R_p=0.013954205377157825\n",
      "Epoch 97: Train loss: 1.3810623660617405 \t Validation loss: 1.4090675201416016 \t R_p=0.01312033899572018\n",
      "Epoch 98: Train loss: 1.326277635998196 \t Validation loss: 1.4571085329055786 \t R_p=0.01328210910977588\n",
      "Epoch 99: Train loss: 1.2855916174782647 \t Validation loss: 1.3942538042068482 \t R_p=0.012919999431714324\n"
     ]
    }
   ],
   "source": [
    "train_epoch_loss = []\n",
    "eval_epoch_loss = []\n",
    "Rp_best = 10\n",
    "for e,epoch in enumerate(range(epochs)):\n",
    "    train_loss = []\n",
    "    eval_loss = []\n",
    "    \n",
    "    l_t = train_epoch(model,train_dl,t0)\n",
    "    train_loss.append(l_t)\n",
    "    \n",
    "    l_e = eval_epoch(model,validation_dl,t0)\n",
    "    eval_loss.append(l_e)\n",
    "    \n",
    "    Rp = test_epoch(model,test_dl,t0)\n",
    "\n",
    "    if Rp_best > Rp:\n",
    "        Rp_best = Rp\n",
    "        \n",
    "    train_epoch_loss.append(np.mean(train_loss))\n",
    "    eval_epoch_loss.append(np.mean(eval_loss))\n",
    "    \n",
    "    print(\"Epoch {}: Train loss: {} \\t Validation loss: {} \\t R_p={}\".format(e,\n",
    "                                                             np.mean(train_loss),\n",
    "                                                             np.mean(eval_loss),Rp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rp best=0.01205371967563964\n"
     ]
    }
   ],
   "source": [
    "print(\"Rp best={}\".format(Rp_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
